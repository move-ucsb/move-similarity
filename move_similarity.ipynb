{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03e651c-18e4-4cfb-993d-0c362bf46771",
   "metadata": {},
   "source": [
    "# Movement similarity\n",
    "\n",
    "https://github.com/move-ucsb/move-similarity\n",
    "\n",
    "Author: Zijian Wan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d0dbb-4e15-4ce8-8425-6245833afe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3d37d-86c2-42d3-b3bd-047ebebfa5e4",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8342d-3a5a-4a8f-b35f-7a1d3444a19d",
   "metadata": {},
   "source": [
    "The pair-wise distances based on various measures, including Frechet, DTW, Hausdorff, LCSS, and NWED, are precomputed using the **trajectory_distance** module, which is available on [GitHub](https://github.com/bguillouet/traj-dist). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c103bdc-ca2a-407f-90c0-9598d1da8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_clustering(metric, n_clu_lowbd, n_clu_highbd, trajs=trajs):\n",
    "    '''\n",
    "    Cluster trajectories using different metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric : str\n",
    "    trajs : list\n",
    "        a list of trajectories, a trajectory is a n*2 np.array (x, y)\n",
    "    n_clu_lowbd : int\n",
    "        lower bound of the number of clusters\n",
    "    n_clu_highbd : TYPE\n",
    "        upper bound of the number of clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels_df : pd.dataframe\n",
    "\n",
    "    '''\n",
    "    n_cols = n_clu_highbd - n_clu_lowbd + 1\n",
    "    \n",
    "    # load the distant array file\n",
    "    file_name = \"mid_migr_lat_30N_tv_\" + metric + \"_arr\"\n",
    "    # open the file for reading\n",
    "    file_object = open(file_name, 'rb')\n",
    "    # load the object from the file into var c\n",
    "    dist_arr = pickle.load(file_object)\n",
    "    \n",
    "    # clustering\n",
    "    labels = np.zeros((len(trajs), n_cols)).astype(int)\n",
    "    count = 0\n",
    "    n_clu_list = []\n",
    "    for n_clusters in range(n_clu_lowbd, n_clu_highbd+1):\n",
    "        cluster = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='average')\n",
    "        clu_results = cluster.fit(dist_arr)\n",
    "        labels[:, count] = clu_results.labels_\n",
    "        count += 1\n",
    "        n_clu_list.append(metric + '_' + str(n_clusters))\n",
    "    \n",
    "    # output\n",
    "    labels_df = pd.DataFrame(data=labels, columns=n_clu_list)\n",
    "    # add 'TrajID' column\n",
    "    trajID = list(range(len(trajs)))\n",
    "    labels_df['TrajID'] = trajID\n",
    "    # rearrange column order\n",
    "    cols = labels_df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    labels_df = labels_df[cols]\n",
    "    \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f15a87-c4d0-4e41-ab3d-7f837d19c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = traj_clustering(metric='frechet', n_clu_lowbd=2, n_clu_highbd=9)\n",
    "cluster_df = copy.deepcopy(labels_df)\n",
    "# dtw\n",
    "labels_df = traj_clustering(metric='dtw', n_clu_lowbd=2, n_clu_highbd=9)\n",
    "cluster_df = cluster_df.merge(labels_df, on='TrajID')\n",
    "# hausdorff\n",
    "labels_df = traj_clustering(metric='hausdorff', n_clu_lowbd=2, n_clu_highbd=9)\n",
    "cluster_df = cluster_df.merge(labels_df, on='TrajID')\n",
    "# lcss\n",
    "labels_df = traj_clustering(metric='lcss', n_clu_lowbd=2, n_clu_highbd=9)\n",
    "cluster_df = cluster_df.merge(labels_df, on='TrajID')\n",
    "# nwed\n",
    "labels_df = traj_clustering(metric='nwed', n_clu_lowbd=2, n_clu_highbd=9)\n",
    "cluster_df = cluster_df.merge(labels_df, on='TrajID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436890b5-1012-4703-ba75-2c390fbaa4a0",
   "metadata": {},
   "source": [
    "Put clustering results based on different distance metrics together for further analysis\n",
    "\n",
    "* Frechet\n",
    "* DTW\n",
    "* Hausdorff\n",
    "* LCSS\n",
    "* NWED\n",
    "\n",
    "Also include turkey vulture id, name, and fall/spring migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c8700-cd7e-42d6-94e3-b389a8e3cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = []\n",
    "names = []\n",
    "migr_status = []\n",
    "fall_begin = datetime(month=10, day=1, year=2000).strftime('%m/%d')\n",
    "fall_end = datetime(month=12, day=5, year=2000).strftime('%m/%d')\n",
    "spring_begin = datetime(month=3, day=15, year=2000).strftime('%m/%d')\n",
    "spring_end = datetime(month=5, day=5, year=2000).strftime('%m/%d')\n",
    "\n",
    "for tji in range(cluster_df.shape[0]):\n",
    "    pti = trajPtID[tji][1][0]\n",
    "    \n",
    "    tags.append(pts_df.loc[pti, 'tag_local_identifier'])\n",
    "    names.append(pts_df.loc[pti, 'individual_local_identifier'])\n",
    "    \n",
    "    pti_dt = pts_df.loc[pti, 'study_local_timestamp']\n",
    "    pti_date = datetime.strptime(pti_dt, '%m/%d/%Y %H:%M:%S').strftime('%m/%d')\n",
    "    if fall_begin <= pti_date <= fall_end:\n",
    "        migr_status.append('Fall')\n",
    "    elif spring_begin <= pti_date <= spring_end:\n",
    "        migr_status.append('Spring')\n",
    "\n",
    "# add columns to cluster_df\n",
    "cluster_df['tag'] = tags\n",
    "cluster_df['name'] = names\n",
    "cluster_df['migr_status'] = migr_status\n",
    "# rearrange column order\n",
    "cols = cluster_df.columns.tolist()\n",
    "cols = cols[-3:] + cols[:-3]\n",
    "cluster_df = cluster_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491ffef-7b63-447a-b648-9761f1a3927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reorder_clu_id(labels, n_clusters):\n",
    "    '''\n",
    "    Reorder the cluster IDs so that the order of the cluster IDs is in the order of\n",
    "    the number of trajectories it contains.\n",
    "        e.g. \n",
    "        cluster ID 0: most\n",
    "        cluster ID 1: second most\n",
    "        ...\n",
    "        cluster ID n: least\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : np.array\n",
    "        labels of clusters\n",
    "    n_clusters : int\n",
    "        number of clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    re_labels : np.array\n",
    "        labels reordered\n",
    "\n",
    "    '''\n",
    "    n_rows = len(labels)\n",
    "    \n",
    "    # count the cluster ids\n",
    "    contrast = np.zeros((n_clusters, 3))\n",
    "    contrast_df = pd.DataFrame(contrast, columns=['clu_id', 'count', 'reordered'])\n",
    "    contrast_df.loc[:, 'clu_id'] = list(range(n_clusters))\n",
    "    for rowi in range(n_rows):\n",
    "        clu_id = labels[rowi]\n",
    "        contrast_df.loc[clu_id, 'count'] = contrast_df.loc[clu_id, 'count'] + 1\n",
    "    \n",
    "    # sort by count\n",
    "    contrast_df = contrast_df.sort_values(by=['count'], ascending=False)\n",
    "    contrast_df.loc[:, 'reordered'] = list(range(n_clusters))\n",
    "    \n",
    "    # output\n",
    "    re_labels = np.zeros(n_rows)\n",
    "    for rowi in range(n_rows):\n",
    "        clu_id_original = labels[rowi]\n",
    "        clu_id_reordered = contrast_df.loc[clu_id_original, 'reordered']\n",
    "        re_labels[rowi] = clu_id_reordered\n",
    "    \n",
    "    re_labels = re_labels.astype('int')\n",
    "    return re_labels\n",
    "    \n",
    "\n",
    "def Reorder_cluster_df(cluster_df, cols):\n",
    "    '''\n",
    "    Reorder a cluster_df\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_df : pd.DataFrame\n",
    "        cluster IDs (and other descriptive information)\n",
    "    cols : list\n",
    "        column numbers (of cluster IDs) that need to be reordered\n",
    "        e.g., [3, 10]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    re_cluster_df : pd.DataFrame\n",
    "        cluster_df reordered \n",
    "\n",
    "    '''\n",
    "    col_names = cluster_df.columns.tolist()\n",
    "    \n",
    "    # output\n",
    "    re_cluster_df = copy.deepcopy(cluster_df)\n",
    "    \n",
    "    # reorder each column\n",
    "    for coli in cols:\n",
    "        # labels (np.array)\n",
    "        labels = cluster_df.iloc[:, coli].to_numpy()\n",
    "        \n",
    "        # number of clusters\n",
    "        col_name = col_names[coli]\n",
    "        n_clu_index = col_name.find('_') + 1\n",
    "        n_clusters = int(col_name[n_clu_index:])\n",
    "        \n",
    "        # reorder the column\n",
    "        re_labels = Reorder_clu_id(labels, n_clusters)\n",
    "        re_cluster_df.iloc[:, coli] = re_labels\n",
    "    \n",
    "    return re_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86211d-0241-441d-a3c8-0e7386641faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(range(4, cluster_df.shape[1]))\n",
    "cluster_df = Reorder_cluster_df(cluster_df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52044939-6b6c-4c59-bb59-cb5607cdc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric_compare(metric, n_clusters, cluster_df=cluster_df):\n",
    "    '''\n",
    "    Examine the trajectories in different clusters.\n",
    "    '''\n",
    "    # find the corresponding trajectories\n",
    "    labels = cluster_df.loc[:, metric].to_numpy()\n",
    "    label_trajs = []\n",
    "    for clui in range(n_clusters):\n",
    "        traj_index = np.where(labels == clui)[0]\n",
    "        label_trajs.append(traj_index)\n",
    "    \n",
    "    avg_n_pts = []  # average number of pts in a trajectory\n",
    "    avg_len_km = []  # average trajectory lengths in kilometers\n",
    "    fall_count = []  # fall migration count\n",
    "    spring_count = []  # spring migration count\n",
    "    avg_start_time = []  # average start time (local)\n",
    "    avg_end_time = []  # average end time (local)\n",
    "    \n",
    "    df_indexes = []  # indexes in the output pd.DataFrame\n",
    "    \n",
    "    for clui in range(n_clusters):\n",
    "        n_trajs = len(label_trajs[clui])  # number of trajectories in the cluster\n",
    "        \n",
    "        n_pts = []\n",
    "        traj_len = []\n",
    "        \n",
    "        # if it is a fall migration trajectory, +1\n",
    "        # if spring, +0\n",
    "        fall_sum = 0\n",
    "        \n",
    "        start_timestamps = []\n",
    "        end_timestamps = []\n",
    "        \n",
    "        for tji in label_trajs[clui]:\n",
    "            # number of points\n",
    "            n_pts.append(trajPtID[tji][2]) \n",
    "            traj_len.append(traj_len_km[tji])\n",
    "            # migration status\n",
    "            if cluster_df.loc[tji, 'migr_status'] == 'Fall':\n",
    "                fall_sum += 1\n",
    "            # start and end time\n",
    "            start_pt = trajPtID[tji][1][0]\n",
    "            end_pt = trajPtID[tji][1][-1]\n",
    "            \n",
    "            real_start_time = datetime.strptime(pts_df.loc[start_pt, 'study_local_timestamp'],\n",
    "                                                '%m/%d/%Y %H:%M:%S')\n",
    "            # year should not be considered\n",
    "            start_time = real_start_time.replace(year=2000)\n",
    "            start_time = datetime.timestamp(start_time)\n",
    "            \n",
    "            real_end_time = datetime.strptime(pts_df.loc[end_pt, 'study_local_timestamp'],\n",
    "                                                '%m/%d/%Y %H:%M:%S')\n",
    "            # year should not be considered\n",
    "            end_time = real_end_time.replace(year=2000)\n",
    "            end_time = datetime.timestamp(end_time)\n",
    "            \n",
    "            start_timestamps.append(start_time)\n",
    "            end_timestamps.append(end_time)\n",
    "        \n",
    "        avg_n_pts.append(round(np.mean(n_pts)))  # average number of points\n",
    "        avg_len_km.append(round(np.mean(traj_len), 3))  # average trajectory lengths in kilometers\n",
    "        fall_count.append(fall_sum)  # fall migration count\n",
    "        spring_count.append(n_trajs - fall_sum)  # spring migration count\n",
    "        avg_start_timestamp = int(np.mean(start_timestamps))\n",
    "        avg_end_timestamp = int(np.mean(end_timestamps))\n",
    "        # average start time\n",
    "        avg_start_time.append(datetime.fromtimestamp(avg_start_timestamp).strftime(\"%m/%d %H:%M:%S\"))\n",
    "        # average end time\n",
    "        avg_end_time.append(datetime.fromtimestamp(avg_end_timestamp).strftime(\"%m/%d %H:%M:%S\"))\n",
    "        \n",
    "        df_indexes.append(metric + '_' + str(clui))   \n",
    "    \n",
    "        new_df = pd.DataFrame(list(zip(avg_n_pts, avg_len_km, fall_count, spring_count,\n",
    "                                         avg_start_time, avg_end_time)),\n",
    "                                         columns = ['avg_n_pts', 'avg_len_km', 'fall_count',\n",
    "                                                    'spring_count', 'avg_start_time',\n",
    "                                                    'avg_end_time'],\n",
    "                                         index=df_indexes)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd15b9-11c6-49ed-81e6-b0a8ecca76a7",
   "metadata": {},
   "source": [
    "### Initial clustering evaluation using Silhoette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bfbae-942d-445d-8f6e-28ab933c2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_silh(mtName, n_clu_lowbd, n_clu_highbd, cluster_df=cluster_df):\n",
    "    '''\n",
    "    Compute the Silhouette Coefficient of the initial clustering result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mtName : str\n",
    "        Name of the metric, e.g., 'frechet'.\n",
    "    n_clu_lowbd : int\n",
    "        The lower bound of the number of clusters, e.g., 2.\n",
    "    n_clu_highbd : int\n",
    "        The upper bound of the number of clusters, e.g., 9.\n",
    "    cluster_df : pd.DataFrame, optional\n",
    "        The dataframe storing the clustering results. The default is cluster_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    silh : float\n",
    "        Computed Silhouette Coefficient.\n",
    "    '''\n",
    "    # read trajectory data\n",
    "    file_name = \"mid_migr_lat_30N_tv_\" + mtName + \"_arr\"\n",
    "    file_object = open(file_name, 'rb')\n",
    "    dist_arr = pickle.load(file_object)\n",
    "    \n",
    "    # normalization\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    dist_one_column = dist_arr.reshape([-1,1])\n",
    "    norm_dist_one_column = min_max_scaler.fit_transform(dist_one_column)\n",
    "    norm_dist_arr = norm_dist_one_column.reshape(dist_arr.shape)\n",
    "    \n",
    "    # read labels \n",
    "    n_clu = []\n",
    "    for n_clui in range(n_clu_lowbd, n_clu_highbd+1):\n",
    "        n_clu.append(mtName + '_' + str(n_clui))\n",
    "    labels = cluster_df.loc[:, n_clu]\n",
    "    \n",
    "    # compute sc\n",
    "    silh = []\n",
    "    for n_clui in range(labels.shape[1]):\n",
    "        silh.append(metrics.silhouette_score(norm_dist_arr, labels.iloc[:, n_clui], \n",
    "                                                 metric='precomputed'))\n",
    "    \n",
    "    return silh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a6053-a7f5-4093-b529-388bb76900e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clu_lowbd = 2\n",
    "n_clu_highbd = 9\n",
    "fre_silh = metric_silh('frechet', n_clu_lowbd, n_clu_highbd, cluster_df)\n",
    "dtw_silh = metric_silh('dtw', n_clu_lowbd, n_clu_highbd, cluster_df)\n",
    "hausdorff_silh = metric_silh('hausdorff', n_clu_lowbd, n_clu_highbd, cluster_df)\n",
    "lcss_silh = metric_silh('lcss', n_clu_lowbd, n_clu_highbd, cluster_df)\n",
    "nwed_silh = metric_silh('nwed', n_clu_lowbd, n_clu_highbd, cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef2c48-3977-4dfe-a741-490bdeb729a6",
   "metadata": {},
   "source": [
    "### Hierachical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8568ca-6378-49e4-b891-0da436dd145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialClui(metric, n_clusters, clui, current_cluster_df=cluster_df):\n",
    "    # initial cluster clui\n",
    "    clusters = []\n",
    "    for ci in range(n_clusters):\n",
    "        traj_index = current_cluster_df[metric + '_' + str(n_clusters)] == ci\n",
    "        clu_trajs = current_cluster_df.loc[traj_index, 'TrajID'].to_list()\n",
    "        n_trajs = len(clu_trajs)\n",
    "        clusters.append([metric + '_' + str(clui), 'n_trajs=' + str(n_trajs), clu_trajs])\n",
    "    \n",
    "    clui_trajs = clusters[clui][2]\n",
    "    \n",
    "    df_idx = []\n",
    "    current_trajs = current_cluster_df.loc[:, 'TrajID'].to_list()\n",
    "    for ci in clui_trajs:\n",
    "        df_idx.append(current_trajs.index(ci))\n",
    "    \n",
    "    clui_df = current_cluster_df.loc[df_idx, ['tag', 'name', 'migr_status', 'TrajID', \n",
    "                                         metric + '_' + str(n_clusters)]].reset_index(\n",
    "                                             drop=True)\n",
    "    clui_df.rename(columns={metric + '_' + str(n_clusters): metric + '-' + str(n_clusters)}, inplace=True)\n",
    "\n",
    "    # add manual labels for later comparison\n",
    "    # label 0-n_tv based on the frequency\n",
    "    tags = clui_df['tag'].to_list()\n",
    "    tags_sorted = sorted(tags, key = tags.count, reverse = True)\n",
    "    tags_unique = []\n",
    "    for tagi in tags_sorted:\n",
    "        if tagi not in tags_unique:\n",
    "            tags_unique.append(tagi)\n",
    "    for count, tagi in enumerate(tags_unique):\n",
    "        clui_df.loc[clui_df['tag'] == tagi, 'tag_label'] =  count\n",
    "    \n",
    "    return clusters, clui_trajs, clui_df\n",
    "\n",
    "\n",
    "def zoomInNCluster(metric, n_clusters, clui, current_cluster_df=cluster_df):\n",
    "    '''\n",
    "    Zoom in cluster\n",
    "    perform clustering again based on the results of initial clustering\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    metric : str\n",
    "        metric of initial clustering, e.g., 'frechet'\n",
    "    n_clusters : int\n",
    "        number of initial clusters \n",
    "    clui : int\n",
    "        cluster id, e.g., 0\n",
    "    current_cluster_df : pd.DataFrame\n",
    "        initial clustering results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cluster_2nd_df : pd.DataFrame\n",
    "        clustering results based on 5 different metrics\n",
    "\n",
    "    '''\n",
    "    clusters, clui_trajs, clui_df = initialClui(metric, n_clusters, clui, current_cluster_df)\n",
    "    \n",
    "    # secondary clustering\n",
    "    # frechet, dtw, hausdorff, lcss, nwed\n",
    "    # n_clusters: 2-9\n",
    "    n_start = 2\n",
    "    \n",
    "    # check n_trajs\n",
    "    n_trajs = len(clui_trajs)\n",
    "    if n_trajs > 9:\n",
    "        n_end = 9\n",
    "    else:\n",
    "        n_end = n_trajs - 1\n",
    "    n_clu_col = 5 * (n_end - n_start + 1)\n",
    "    \n",
    "    labels = np.zeros((len(clui_trajs), n_clu_col)).astype(int)\n",
    "    dist_metrics = ['frechet', 'dtw', 'hausdorff', 'lcss', 'nwed']\n",
    "    n_clu_list = []  # column names\n",
    "    count = 0\n",
    "    \n",
    "    for meti in dist_metrics:\n",
    "        # read precomputed distance array\n",
    "        file_name = \"mid_migr_lat_30N_tv_\" + meti + \"_arr\"\n",
    "        file_object = open(file_name, 'rb')\n",
    "        dist_arr = pickle.load(file_object)\n",
    "        \n",
    "        # new dist_arr contains only trajectories in clui_df\n",
    "        clui_trajs = clusters[clui][2]\n",
    "        clui_dist_arr = dist_arr[clui_trajs, :][:, clui_trajs]\n",
    "        \n",
    "        for sec_n_clu in range(n_start, n_end+1):\n",
    "            # column names\n",
    "            n_clu_list.append(meti + '_' + str(sec_n_clu))\n",
    "            \n",
    "            cluster = AgglomerativeClustering(n_clusters=sec_n_clu, \n",
    "                                              affinity='precomputed', linkage='complete')\n",
    "            clu_results = cluster.fit(clui_dist_arr)\n",
    "            labels[:, count] = clu_results.labels_\n",
    "            count += 1\n",
    "    \n",
    "    # output pd.DataFrame\n",
    "    cluster_2nd_df = pd.DataFrame(data=labels, columns=n_clu_list)\n",
    "    # add tag, name, migr_status, TrajID to cluster_2nd_df\n",
    "    cluster_2nd_df = clui_df.join(cluster_2nd_df)\n",
    "    \n",
    "    return cluster_2nd_df\n",
    "\n",
    "def cmptSihouette(cluster_2nd_nc_df, clui):\n",
    "    start_col = 6  # start with the 6th column  \n",
    "    col_names = cluster_2nd_nc_df.columns.tolist()\n",
    "    \n",
    "    sih_score_list = []\n",
    "    for coli in range(start_col, cluster_2nd_nc_df.shape[1]):     \n",
    "        # (2nd) metric and n_clusters\n",
    "        col_name = col_names[coli]\n",
    "        index_ = col_name.index('_')\n",
    "        metric_2nd = col_name[:index_]\n",
    "        n_clusters_2nd = int(col_name[(index_+1):])\n",
    "        \n",
    "        # distance array\n",
    "        # read precomputed distance array\n",
    "        file_name = \"mid_migr_lat_30N_tv_\" + metric_2nd + \"_arr\"\n",
    "        file_object = open(file_name, 'rb')\n",
    "        dist_arr = pickle.load(file_object)\n",
    "        # new dist_arr contains only trajectories in clui_df\n",
    "        clui_trajs = cluster_2nd_nc_df.loc[:, 'TrajID'].to_list()\n",
    "        clui_dist_arr = dist_arr[clui_trajs, :][:, clui_trajs]\n",
    "    \n",
    "        # compute sihouette score\n",
    "        labels = cluster_2nd_nc_df.iloc[:, coli]\n",
    "        sih_score = metrics.silhouette_score(clui_dist_arr, labels, metric='precomputed')\n",
    "        sih_score_list.append(sih_score)\n",
    "    \n",
    "    # arrange the silhouette score list\n",
    "    sih_score_list_arranged = []\n",
    "    dist_metrics = ['frechet', 'dtw', 'hausdorff', 'lcss', 'nwed']\n",
    "    # number of clusters tried\n",
    "    n_metrics = 5\n",
    "    n_clu_trials = int((cluster_2nd_nc_df.shape[1] - start_col) / n_metrics)\n",
    "    for meti in range(len(dist_metrics)):\n",
    "        new_list = [dist_metrics[meti], sih_score_list[meti*n_clu_trials:(meti+1)*n_clu_trials]]\n",
    "        sih_score_list_arranged.append(new_list)\n",
    "    \n",
    "    return sih_score_list_arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720e9e9-b522-4234-8b8c-030086785176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSihZoomInCluster(metric, init_n_clu, clui, current_cluster_df):\n",
    "    # zoom-in clustering\n",
    "    data_df = zoomInNCluster(metric=metric, n_clusters=init_n_clu, clui=clui, \n",
    "                             current_cluster_df=current_cluster_df)\n",
    "    data_reorder_df = Reorder_cluster_df(data_df, list(range(6, data_df.shape[1])))\n",
    "    \n",
    "    # compute sihouette coefficient\n",
    "    sih = cmptSihouette(data_reorder_df, clui=clui)\n",
    "    \n",
    "    # find the metric with the highest sihouette coefficient\n",
    "    metric_highest_sih = []\n",
    "    for meti in sih:\n",
    "        metric_highest_sih.append(max(meti[1]))\n",
    "    high_mt_idx = metric_highest_sih.index(max(metric_highest_sih))\n",
    "    if high_mt_idx == 0:\n",
    "        high_metric = 'frechet'\n",
    "    elif high_mt_idx == 1:\n",
    "        high_metric = 'dtw'\n",
    "    elif high_mt_idx == 2:\n",
    "        high_metric = 'hausdorff'\n",
    "    elif high_mt_idx == 3:\n",
    "        high_metric = 'lcss'\n",
    "    elif high_mt_idx == 4:\n",
    "        high_metric = 'nwed'\n",
    "    # n_clusters starts with 2\n",
    "    high_n_clu = sih[high_mt_idx][1].index(metric_highest_sih[high_mt_idx]) + 2\n",
    "    \n",
    "    # clustering results that have the highest sihouette coefficient\n",
    "    col_names = data_reorder_df.columns.to_list()[:6]\n",
    "    mt_highest_sih = high_metric + '_' + str(high_n_clu)\n",
    "    col_names.append(mt_highest_sih)\n",
    "    high_sih_df = data_reorder_df.loc[:, col_names]\n",
    "    \n",
    "    # highest sihouette\n",
    "    highest_sih = max(metric_highest_sih)\n",
    "    \n",
    "    return high_sih_df, mt_highest_sih, highest_sih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc48f3-1c22-47c1-9787-bb8812f56138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurCluster_SC(current_cluster_df):\n",
    "    # parameters for zoom-in clustering\n",
    "    prev_mt_highest_sih = current_cluster_df.columns.to_list()[6]\n",
    "    index_ = prev_mt_highest_sih.index('_')\n",
    "    metric = prev_mt_highest_sih[:index_]\n",
    "    init_n_clu = int(prev_mt_highest_sih[(index_+1):])\n",
    "    clui = current_cluster_df.loc[0, prev_mt_highest_sih]\n",
    "    \n",
    "    # zoom-in clustering\n",
    "    high_sih_df, mt_highest_sih, highest_sih = bestSihZoomInCluster(metric, init_n_clu, \n",
    "                                                       clui, current_cluster_df)\n",
    "    index_ = mt_highest_sih.index('_')\n",
    "    high_n_clu = int(mt_highest_sih[(index_+1):])\n",
    "    \n",
    "    # update output_list\n",
    "    output_list_update = []\n",
    "    for ci in range(high_n_clu):\n",
    "        output_list_update.append(high_sih_df.loc[high_sih_df[mt_highest_sih] == ci]\n",
    "                                  .reset_index(drop=True))\n",
    "    \n",
    "    return output_list_update, highest_sih, mt_highest_sih\n",
    "\n",
    "def hrchyCluster_SC(dist_met, init_n_clu, clui, current_cluster_df=cluster_df):\n",
    "    # sihouette records for each step of clustering\n",
    "    sih_records = []\n",
    "    # metric records\n",
    "    met_records = []\n",
    "    \n",
    "    # zoom-in clustering\n",
    "    high_sih_df, mt_highest_sih, highest_sih = bestSihZoomInCluster(dist_met, \n",
    "                                                                    init_n_clu, \n",
    "                                                                    clui, current_cluster_df)\n",
    "    index_ = mt_highest_sih.index('_')\n",
    "    high_n_clu = int(mt_highest_sih[(index_+1):])\n",
    "    sih_records.append(highest_sih)\n",
    "    met_records.append(mt_highest_sih)\n",
    "    \n",
    "    # output\n",
    "    output_records = []\n",
    "    output_list = []\n",
    "    for ci in range(high_n_clu):\n",
    "        output_list.append(high_sih_df.loc[high_sih_df[mt_highest_sih] == ci]\n",
    "                           .reset_index(drop=True))\n",
    "    hrchyi_list = []  # clusters of trajectories at that hierarchy\n",
    "    for li in output_list:\n",
    "        hrchyi_list.append(li.loc[:, 'TrajID'].to_list())\n",
    "    output_records.append(hrchyi_list)\n",
    "    \n",
    "    # check the highest SC\n",
    "    # to decide whether to continue zoom-in clustering\n",
    "    global_highest_sih = highest_sih\n",
    "    while global_highest_sih > sc_th:\n",
    "        global_highest_sih = -1  # the highest silh at that hierarchy\n",
    "        \n",
    "        new_hrchyi_sih_records = []\n",
    "        new_hrchyi_met_records = []\n",
    "        \n",
    "        # output list will change later\n",
    "        orig_output_list = copy.deepcopy(output_list)\n",
    "        # check whether the cluster needs further clustering\n",
    "        updated_output_list = []\n",
    "        for clui_id, clui_trajs in enumerate(hrchyi_list):\n",
    "            if len(clui_trajs) <= 2:\n",
    "                updated_output_list.append(output_list[clui_id])\n",
    "                continue\n",
    "            \n",
    "            current_cluster_df = orig_output_list[clui_id]\n",
    "            # zoom-in clustering\n",
    "            output_list_update, highest_sih, mt_highest_sih = recurCluster_SC(current_cluster_df)\n",
    "            \n",
    "            # check whether this further clustering makes sense\n",
    "            if highest_sih >= sc_th:  # clustering makes sense\n",
    "                # update output_list\n",
    "                updated_output_list.extend(output_list_update)\n",
    "                \n",
    "                # record silh and met\n",
    "                new_hrchyi_sih_records.append(highest_sih)\n",
    "                new_hrchyi_met_records.append(mt_highest_sih)\n",
    "            \n",
    "            else: \n",
    "                updated_output_list.append(output_list[clui_id])\n",
    "        \n",
    "            # update the global_highest_sih\n",
    "            if highest_sih > global_highest_sih:\n",
    "                global_highest_sih = highest_sih\n",
    "        \n",
    "        output_list = copy.deepcopy(updated_output_list)\n",
    "        \n",
    "        # chech whether the new hierarchy needs to be recorded\n",
    "        if global_highest_sih >= sc_th:  \n",
    "            # record trajs at that hierarchy\n",
    "            new_hrchyi_list = []\n",
    "            for li in output_list:\n",
    "                new_hrchyi_list.append(li.loc[:, 'TrajID'].to_list())\n",
    "            output_records.append(new_hrchyi_list)\n",
    "        \n",
    "            # record silh and met at that hierarchy\n",
    "            sih_records.append(new_hrchyi_sih_records)\n",
    "            met_records.append(new_hrchyi_met_records)\n",
    "        \n",
    "        # update the hierarchy list to begin the next iteration\n",
    "        hrchyi_list = copy.deepcopy(new_hrchyi_list)\n",
    "    \n",
    "    return output_records, sih_records, met_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3873a-2bde-4bfc-a96f-dcdd1bd0daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_th = 0.5\n",
    "\n",
    "frechet_2_0_hrchy, frechet_2_0_hrchy_sih, frechet_2_0_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='frechet', init_n_clu=2, clui=0, current_cluster_df=cluster_df)\n",
    "frechet_2_1_hrchy, frechet_2_1_hrchy_sih, frechet_2_1_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='frechet', init_n_clu=2, clui=1, current_cluster_df=cluster_df)\n",
    "\n",
    "dtw_3_0_hrchy, dtw_3_0_hrchy_sih, dtw_3_0_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='dtw', init_n_clu=3, clui=0, current_cluster_df=cluster_df)\n",
    "dtw_3_1_hrchy, dtw_3_1_hrchy_sih, dtw_3_1_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='dtw', init_n_clu=3, clui=1, current_cluster_df=cluster_df)\n",
    "\n",
    "hausdorff_2_0_hrchy, hausdorff_2_0_hrchy_sih, hausdorff_2_0_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='hausdorff', init_n_clu=2, clui=0, current_cluster_df=cluster_df)\n",
    "\n",
    "lcss_3_0_hrchy, lcss_3_0_hrchy_sih, lcss_3_0_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='lcss', init_n_clu=3, clui=0, current_cluster_df=cluster_df)\n",
    "lcss_3_1_hrchy, lcss_3_1_hrchy_sih, lcss_3_1_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='lcss', init_n_clu=3, clui=1, current_cluster_df=cluster_df)\n",
    "\n",
    "nwed_2_0_hrchy, nwed_2_0_hrchy_sih, nwed_2_0_hrchy_met = hrchyCluster_SC(\n",
    "    dist_met='nwed', init_n_clu=2, clui=0, current_cluster_df=cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ecf06-faa3-44b5-b926-8bfccff7e368",
   "metadata": {},
   "source": [
    "### Characterize clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbc295-94f0-4d17-aaf5-84a9c12ff067",
   "metadata": {},
   "source": [
    "Try to interpret the clustering results with environmental variables obtained from Movebank.  Eamaine the clusters that are mostly agreed on, i.e., the results obtained starting with Frechet or DTW ro_up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce83fa-5a4d-4555-94fd-e6edbb5431dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov (KS) test\n",
    "env_vars = ['Movebank Thermal Uplift (from ECMWF)',\n",
    "            'ECMWF Interim Full Daily SFC Temperature (2 m above Ground)',\n",
    "            'MODIS Land Vegetation Indices 250m 16d Aqua NDVI',\n",
    "            'Movebank Orographic Uplift (from ASTER DEM and ECMWF)',      \n",
    "            'ECMWF ERA5 SL Wind (10 m above Ground U Component)',\n",
    "            'ECMWF ERA5 SL Wind (10 m above Ground V Component)',\n",
    "            'ECMWF ERA5 SL Total Precipitation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36c6d1-8c84-406a-a98c-90aa589b1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clu_Env_KS_test(cluster_lables, env_var_idx, migr_state, pts_df=pts_df, sim_pts_df=sim_pts_df):\n",
    "    '''\n",
    "    Compare the distribution of env vars in sim pts and org pts for each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cluster_lables : list\n",
    "        The last element of it is the hierarchical clustering result.\n",
    "    env_var_idx : list\n",
    "        A list of environmental variable indexes.\n",
    "    migr_state : str\n",
    "        Fall or spring migration\n",
    "    pts_df : pd.df, optional\n",
    "        Original pts. The default is pts_df.\n",
    "    sim_pts_df : pd.df, optional\n",
    "        Simulation (background) pts. The default is sim_pts_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ks_results : list\n",
    "        A list of test results together with the environmental variable names.\n",
    "        e.g., [env_var1, KstestResult(statistic=0.444356027159..., pvalue=0.038850140086...)]\n",
    "    '''\n",
    "    # environmental variables\n",
    "    env_vars = [pts_df.columns.to_list()[i] for i in env_var_idx]\n",
    "    \n",
    "    clu_trajs = []  # all clustered trajectories\n",
    "    for clui in cluster_lables:\n",
    "        clu_trajs.extend(clui)\n",
    "    clu_pts = []  # all pts in clustered trajectories\n",
    "    for tji in clu_trajs:\n",
    "        clu_pts.extend(trajPtID[tji][1])\n",
    "    \n",
    "    ks_results = []  # output\n",
    "    \n",
    "    for clu_count, labelj in enumerate(cluster_lables):\n",
    "        # KS test output\n",
    "        d_stats = np.zeros((len(env_vars), 2))\n",
    "        p_values = np.zeros((len(env_vars), 2))\n",
    "        \n",
    "        # trajectory points in the cluster\n",
    "        labelj_pts = []\n",
    "        for traji in labelj:\n",
    "            labelj_pts.extend(trajPtID[traji][1])\n",
    "        \n",
    "        # cluster years\n",
    "        clu_years = []\n",
    "        for tji in clu_trajs:\n",
    "            tji_pt0 = trajPtID[tji][1][0]\n",
    "            pti_dt_obj = datetime.strptime(pts_df.loc[tji_pt0, 'study_local_timestamp'],\n",
    "                               '%m/%d/%Y %H:%M:%S')\n",
    "            pts_year = pti_dt_obj.year\n",
    "            if pts_year not in clu_years:\n",
    "                clu_years.append(pts_year)\n",
    "        clu_years.sort()\n",
    "        \n",
    "        # simulated points (background) year\n",
    "        clu_year_idx = [years.index(i) for i in clu_years]\n",
    "        sim_pts_in_clu_years = []\n",
    "        for yeari in clu_year_idx:\n",
    "            sim_pts_in_clu_years.extend(year_sim_pts[yeari][1])\n",
    "        clu_sim_pts_df = sim_pts_df.loc[sim_pts_in_clu_years]\n",
    "        \n",
    "        for env_count, envi in enumerate(env_vars):\n",
    "            # values of the environmental variable of the points in the cluster\n",
    "            labelj_envi = pts_df.loc[labelj_pts, envi].to_numpy()\n",
    "            labelj_envi = labelj_envi[~np.isnan(labelj_envi)]  # remove nan\n",
    "            \n",
    "            if len(labelj_envi) == 0:  # missing values\n",
    "                continue\n",
    "            \n",
    "            # simulated points (background) season\n",
    "            clu_season_bool = clu_sim_pts_df['Migr_state'] == migr_state\n",
    "            clu_season_sim_pts_df = clu_sim_pts_df.loc[clu_season_bool]\n",
    "            sim_envi = clu_season_sim_pts_df.loc[:, envi].to_numpy()\n",
    "            sim_envi = sim_envi[~np.isnan(sim_envi)]  # remove nan\n",
    "        \n",
    "            # KS test with the background\n",
    "            labelj_envi_ks = stats.kstest(labelj_envi, sim_envi, N=len(labelj))\n",
    "            d_stats[env_count, 0] = labelj_envi_ks[0]\n",
    "            p_values[env_count, 0] = labelj_envi_ks[1]\n",
    "           \n",
    "        # results\n",
    "        clu_ks_df = pd.DataFrame({\n",
    "            'cluster': [(clu_count+1) for i in range(len(env_vars))],\n",
    "            'env_var': env_vars,\n",
    "            'stat_back': d_stats[:,0],\n",
    "            'p-value_back': p_values[:, 0]\n",
    "                               })\n",
    "        ks_results.append([labelj, clu_ks_df])\n",
    "    \n",
    "    return ks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bf71e-03b4-4263-a4e6-378d78f5a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_ks_results = Clu_Env_KS_test(cluster_lables=dtw_3_0_hrchy[-1], \n",
    "                                  env_var_idx=[19, 22, 21, 20, 25, 26, 27], \n",
    "                                  migr_state='Fall')\n",
    "spring_ks_results = Clu_Env_KS_test(cluster_lables=dtw_3_1_hrchy[-1], \n",
    "                                    env_var_idx=[19, 22, 21, 20, 25, 26, 27], \n",
    "                                    migr_state='Spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55d8aa-55eb-4c64-90c1-b46aa8d96fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pair_Env_KS_test(cluster_lables, env_var_idx, migr_state, pts_df=pts_df, sim_pts_df=sim_pts_df):\n",
    "    '''\n",
    "    Pairwise KS test\n",
    "    '''\n",
    "    # environmental variables\n",
    "    env_vars = [pts_df.columns.to_list()[i] for i in env_var_idx]\n",
    "    \n",
    "    clu_trajs = []  # all clustered trajectories\n",
    "    for clui in cluster_lables:\n",
    "        clu_trajs.extend(clui)\n",
    "    clu_pts = []  # all pts in clustered trajectories\n",
    "    for tji in clu_trajs:\n",
    "        clu_pts.extend(trajPtID[tji][1])\n",
    "    \n",
    "    ks_results = []  # output\n",
    "    \n",
    "    for envi in env_vars:\n",
    "        # KS test output\n",
    "        p_values = np.ones((len(cluster_lables), len(cluster_lables)))\n",
    "        \n",
    "        for clu_count, labelj in enumerate(cluster_lables):\n",
    "            # trajectory points in the cluster\n",
    "            labelj_pts = []\n",
    "            for traji in labelj:\n",
    "                labelj_pts.extend(trajPtID[traji][1])\n",
    "            \n",
    "            # values of the environmental variable of the points in the cluster\n",
    "            labelj_envi = pts_df.loc[labelj_pts, envi].to_numpy()\n",
    "            labelj_envi = labelj_envi[~np.isnan(labelj_envi)]  # remove nan\n",
    "            \n",
    "            if len(labelj_envi) == 0:  # missing values\n",
    "                continue\n",
    "            \n",
    "            for clu_count2, labelk in enumerate(cluster_lables):\n",
    "                if clu_count == clu_count2:\n",
    "                    continue\n",
    "                \n",
    "                # trajectory points in the cluster\n",
    "                labelk_pts = []\n",
    "                for traji in labelk:\n",
    "                    labelk_pts.extend(trajPtID[traji][1])\n",
    "                \n",
    "                # values of the environmental variable of the points in the cluster\n",
    "                labelk_envi = pts_df.loc[labelk_pts, envi].to_numpy()\n",
    "                labelk_envi = labelk_envi[~np.isnan(labelk_envi)]  # remove nan\n",
    "                \n",
    "                if len(labelk_envi) == 0:  # missing values\n",
    "                    continue\n",
    "                \n",
    "                # KS test with other clusters\n",
    "                labelj_k_ks = stats.kstest(labelj_envi, labelk_envi, N=len(labelj))\n",
    "                p_values[clu_count, clu_count2] = labelj_k_ks[1]\n",
    "        \n",
    "        ks_results.append([envi, p_values])\n",
    "    \n",
    "    return ks_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bca785-0f14-4723-8797-65739172ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fall_pair_ks = Pair_Env_KS_test(cluster_lables=dtw_3_0_hrchy[-1], \n",
    "                                  env_var_idx=[19, 22, 21, 20, 25, 26, 27], \n",
    "                                  migr_state='Fall')\n",
    "spring_pair_ks = Pair_Env_KS_test(cluster_lables=dtw_3_1_hrchy[-1], \n",
    "                                  env_var_idx=[19, 22, 21, 20, 25, 26, 27], \n",
    "                                  migr_state='Spring')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1108dad-b469-4163-9ae1-c1b07d6e18ed",
   "metadata": {},
   "source": [
    "#### Jensen-Shannon distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337b937-e769-4fc9-8314-7099d36965c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def btw_clu_jsd(cluster_lables, env_var_idx):\n",
    "    '''\n",
    "    Between-cluster JSD\n",
    "    '''\n",
    "    env_vars = [pts_df.columns.to_list()[i] for i in env_var_idx]\n",
    "    jsd_output = []\n",
    "    \n",
    "    for envi in env_vars:\n",
    "        jsd_mat = np.zeros((len(cluster_lables), len(cluster_lables)))\n",
    "        \n",
    "        for clu_count, labelj in enumerate(cluster_lables):\n",
    "            # trajectory points in the cluster\n",
    "            labelj_pts = []\n",
    "            for traji in labelj:\n",
    "                labelj_pts.extend(trajPtID[traji][1])\n",
    "\n",
    "            # values of the environmental variable of the points in the cluster\n",
    "            labelj_envi = pts_df.loc[labelj_pts, envi].to_numpy()\n",
    "            # remove nan\n",
    "            labelj_envi = labelj_envi[~np.isnan(labelj_envi)]\n",
    "            \n",
    "            if len(labelj_envi) == 0:\n",
    "                continue\n",
    "            \n",
    "            # range\n",
    "            hist_range = (min(labelj_envi), max(labelj_envi))\n",
    "            \n",
    "            # probability vector\n",
    "            labelj_envi_prob_vec = np.histogram(labelj_envi, bins=100, range=hist_range)[0] / len(labelj_envi)\n",
    "            \n",
    "            for clu_count2, labelk in enumerate(cluster_lables):\n",
    "                # check whether they are the same cluster\n",
    "                if clu_count2 == clu_count:\n",
    "                    continue\n",
    "                \n",
    "                # trajectory points in cluster k\n",
    "                labelk_pts = []\n",
    "                for traji in labelk:\n",
    "                    labelk_pts.extend(trajPtID[traji][1])\n",
    "                labelk_envi = pts_df.loc[labelk_pts, envi].to_numpy()\n",
    "                \n",
    "                # remove nan\n",
    "                labelk_envi = labelk_envi[~np.isnan(labelk_envi)]\n",
    "                \n",
    "                if len(labelk_envi) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # probability vector\n",
    "                labelk_envi_prob_vec = np.histogram(labelk_envi, bins=100, range=hist_range)[0] / len(labelk_envi)\n",
    "                # update jsd_mat\n",
    "                jsd_jk = distance.jensenshannon(labelj_envi_prob_vec, labelk_envi_prob_vec)\n",
    "                jsd_mat[clu_count, clu_count2] = jsd_jk\n",
    "                jsd_mat[clu_count2, clu_count] = jsd_jk\n",
    "            \n",
    "        envi_list = [envi, jsd_mat]\n",
    "        jsd_output.append(envi_list)\n",
    "    \n",
    "    return jsd_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a02875-8697-4778-8efc-fbbddb68f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_var_idx = [19, 22, 21, 20, 25, 26, 27]\n",
    "spring_jsd = btw_clu_jsd(cluster_lables=spring_clusters, env_var_idx=env_var_idx)\n",
    "\n",
    "temp_mean_jsd = np.mean(spring_jsd[3][1][np.nonzero(spring_jsd[3][1])])\n",
    "ndvi_mean_jsd = np.mean(spring_jsd[2][1][np.nonzero(spring_jsd[2][1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d71a76-8c97-4726-af06-d680e2a539a7",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54612c1-b7a5-4c88-88f6-3c24cf5d2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55ec93-57ce-4c27-8593-8f169ebc8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHrchyCluter_SC(hrchy_results, fig_title):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    x = [str(item) for sublist in hrchy_results[-1] for item in sublist]\n",
    "    plt.plot(x, [0 for item in x], color=(0,0,0,0))  # transparent\n",
    "\n",
    "    clus = []  # store the clusters\n",
    "    for count, hrchyi in enumerate(hrchy_results):\n",
    "        ci_y = len(hrchy_results) - count - 1\n",
    "        for ci in hrchyi:\n",
    "            if len(ci) == 1:  # only 1 trajectory\n",
    "                plt.vlines(str(ci[0]), ci_y, ci_y+1, color='blue')\n",
    "                clus.append(ci)\n",
    "            else:\n",
    "                # find the index in the string list\n",
    "                x_idx = np.zeros((len(ci), 2)).astype(int)\n",
    "                x_idx[:, 0] = ci\n",
    "                for rowi in range(len(x_idx)):\n",
    "                    x_idx[rowi, 1] = x.index(str(x_idx[rowi, 0]))\n",
    "                x_idx_sort = x_idx[np.argsort(x_idx[:, 1])]\n",
    "\n",
    "                plt.vlines(str(x_idx_sort[0, 0]), ci_y, ci_y+1, color='blue')\n",
    "                plt.vlines(str(x_idx_sort[-1, 0]), ci_y, ci_y+1, color='blue')\n",
    "\n",
    "                if ci not in clus:\n",
    "                    ci_x = [str(item) for item in ci]\n",
    "                    ci_y_list = [ci_y+1 for item in ci]\n",
    "                    plt.plot(ci_x, ci_y_list, color='blue')\n",
    "                    clus.append(ci)\n",
    "    \n",
    "    # the overarching cluster\n",
    "    plt.vlines(x[0], len(hrchy_results), len(hrchy_results)+1, color='blue')\n",
    "    plt.vlines(x[-1], len(hrchy_results), len(hrchy_results)+1, color='blue')\n",
    "    plt.plot(x, [len(hrchy_results)+1 for item in x], color='blue')\n",
    "    \n",
    "\n",
    "    yticks = range(len(hrchy_results)+2)\n",
    "    plt.yticks(yticks, yticks[::-1])\n",
    "    plt.xlabel('Trajectory ID')\n",
    "    plt.ylabel('Hierarchy')\n",
    "    plt.title(fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0e5fd-68d8-428a-8d34-9e536bd2ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHrchyCluter_SC(frechet_2_0_hrchy, 'frechet_2_0')\n",
    "plotHrchyCluter_SC(frechet_2_1_hrchy, 'frechet_2_1')\n",
    "\n",
    "plotHrchyCluter_SC(dtw_3_0_hrchy, 'dtw_3_0')\n",
    "plotHrchyCluter_SC(dtw_3_1_hrchy, 'dtw_3_1')\n",
    "\n",
    "plotHrchyCluter_SC(hausdorff_2_0_hrchy, 'hausdorff_2_0')\n",
    "\n",
    "plotHrchyCluter_SC(lcss_3_0_hrchy, 'lcss_3_0')\n",
    "plotHrchyCluter_SC(lcss_3_1_hrchy, 'lcss_3_1')\n",
    "\n",
    "plotHrchyCluter_SC(nwed_2_0_hrchy, 'nwed_2_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa19f2-e58c-4332-bc03-eb84042160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_env_distr_bw(cluster_lables, env_var_idx, migr_state, bw=0.3, pts_df=pts_df, sim_pts_df=sim_pts_df):\n",
    "    '''\n",
    "    Plot the distribution of the environmental variables of vairous cluters \n",
    "        with simulated points as the baseline.\n",
    "    '''\n",
    "    env_vars = [pts_df.columns.to_list()[i] for i in env_var_idx]\n",
    "    n_env_vars = len(env_vars)\n",
    "    nrows = np.ceil(n_env_vars / 2)\n",
    "    count = 1\n",
    "    \n",
    "    for envi in env_vars:\n",
    "        plt.subplot(nrows, 2, count)\n",
    "        \n",
    "        # range of x\n",
    "        xmin = min(pts_df[envi])\n",
    "        xmax = max(pts_df[envi])\n",
    "        dx = 0.2 * (xmax - xmin) # add a 20% margin, as the kde is wider than the data\n",
    "        xmin -= dx\n",
    "        xmax += dx\n",
    "        x = np.linspace(xmin, xmax, 500)\n",
    "        \n",
    "        # simulated points\n",
    "        if migr_state == 'Fall':\n",
    "            fall_sim_pts_df = sim_pts_df.loc[sim_pts_df['Migr_state'] == 'Fall']\n",
    "            sim_envi = fall_sim_pts_df.loc[:, envi].to_numpy()\n",
    "        elif migr_state == 'Spring':\n",
    "            spring_sim_pts_df = sim_pts_df.loc[sim_pts_df['Migr_state'] == 'Spring']\n",
    "            sim_envi = spring_sim_pts_df.loc[:, envi].to_numpy()\n",
    "        \n",
    "        # remove nan\n",
    "        sim_envi = sim_envi[~np.isnan(sim_envi)]\n",
    "        # kernel\n",
    "        sim_kernel = stats.gaussian_kde(sim_envi, bw_method=bw)\n",
    "        # KDE\n",
    "        kde_back_x = sim_kernel(x)\n",
    "\n",
    "        for clu_count, labelj in enumerate(cluster_lables):\n",
    "            # trajectory points in the cluster\n",
    "            labelj_pts = []\n",
    "            for traji in labelj:\n",
    "                labelj_pts.extend(trajPtID[traji][1])\n",
    "\n",
    "            # values of the environmental variable of the points in the cluster\n",
    "            labelj_envi = pts_df.loc[labelj_pts, envi].to_numpy()\n",
    "            labelj_envi = labelj_envi[~np.isnan(labelj_envi)]  # remove nan\n",
    "            \n",
    "            if len(labelj_envi) == 0:\n",
    "                continue\n",
    "    \n",
    "            # kernel j\n",
    "            kernelj = stats.gaussian_kde(labelj_envi, bw_method=bw)\n",
    "            kdej_x = kernelj(x)\n",
    "\n",
    "            # density plot\n",
    "            color_palette = sns.color_palette(\"Paired\")\n",
    "            if clu_count < 12:\n",
    "                color =  color_palette[clu_count] # RGB alpha\n",
    "            else:\n",
    "                count12 = clu_count - 12\n",
    "                color_palette = sns.color_palette(\"Set2\")\n",
    "                color =  color_palette[count12]\n",
    "            plt.plot(x, kdej_x, color=color, label='Cluster_'+str(clu_count+1))\n",
    "\n",
    "        plt.xlabel(envi)\n",
    "        \n",
    "        if count % 2 == 1:\n",
    "            plt.ylabel(\"Probability density\")\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    labels, ids = np.unique(labels, return_index=True)\n",
    "    sort_idx = np.argsort(ids)\n",
    "    ids = ids[sort_idx]\n",
    "    labels = labels[sort_idx]\n",
    "    handles = [handles[i] for i in ids]\n",
    "    plt.figlegend(labels, loc='center right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a6c3c-91ff-4e09-afc1-62afff9d60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_var_idx = [19, 22, 21, 20, 25, 26, 27]\n",
    "\n",
    "# fall migration\n",
    "plt.figure(figsize=(16,12))\n",
    "Plot_env_distr_bw(dtw_3_0_hrchy[-1], env_var_idx, migr_state='Fall', bw=0.3)\n",
    "# plt.suptitle(\"Fall migration\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.subplots_adjust(wspace=0.11)\n",
    "plt.subplot(4,2,5)  # precipitation\n",
    "plt.xlim(-0.001, 0.002)\n",
    "plt.suptitle('Fall migration')\n",
    "\n",
    "# spring migration\n",
    "plt.figure(figsize=(16,12))\n",
    "Plot_env_distr_bw(dtw_3_1_hrchy[-1], env_var_idx, migr_state='Spring', bw=0.3)\n",
    "# plt.suptitle(\"Spring migration\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.subplots_adjust(wspace=0.11)\n",
    "plt.subplot(4,2,5)\n",
    "plt.xlim(-0.001, 0.002)\n",
    "plt.suptitle('Spring migration')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpd)",
   "language": "python",
   "name": "gpd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
